Title:
CLIP-Dissect Automatic Evaluation

Abstract:
Network dissecting is a method used to understand the inner workings of deep neural networks by examining the functionality of individual neurons. It typically involves techniques to automatically associate neurons with specific concepts or features, providing insights into how the network processes information. CLIP-Dissect, a novel method, stands out for its capability to automatically label internal neurons with concepts, leveraging multimodal vision/language models such as CLIP. The output of CLIP-Dissect entails the labeling of individual neurons with associated concepts, elucidating the network's functioning. In the CLIP Dissect paper, quantitative evaluation for neuron labels has been done in terms of final layer neuron evaluations due to having access to the neuron ground truths. However, for hidden neurons, the inaccessible ground truths make quantitative evaluation harder. Qualitative observations have been done, including looking at small subsets of hidden neuron labels and their activating images as evaluation, however this is a manual process that requires careful observation. In this study we introduce a way to replicate this style of evaluation automatically. To do this, we utilize 3 different approaches of observing activating images and their labels. We find in this study that utilizing pretrained and benchmarked models to do this task is an effective method to conduct qualitative assessment of neuron dissections. The methods used are prompting a VQA (BLIP-2), and using OpenCLIP embedding similarities as means of evaluation. Notably, BLIP-2 demonstrated a high alignment with human evaluations, achieving an Intersection over Union (IoU) score of 0.809, surpassing the OpenCLIP methods which recorded IoUs of 0.768 and 0.776. 
